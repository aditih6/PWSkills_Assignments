{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59747dc-7d8f-4930-8988-9e1e011c6166",
   "metadata": {},
   "source": [
    "# SVM and Naive Bayes Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed38b6-133a-4f63-bfbd-0b74e02aafdb",
   "metadata": {},
   "source": [
    "1.  What is a Support Vector Machine (SVM), and how does it work?\n",
    "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks (though it‚Äôs mostly used for classification).\n",
    "\n",
    "At its core, SVM aims to find the best decision boundary‚Äîcalled a hyperplane‚Äîthat separates data points of different classes with the maximum possible margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45b7bc-58c8-4c41-bbcb-80cae7c7addc",
   "metadata": {},
   "source": [
    "2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
    "A Hard Margin SVM assumes that the data is perfectly linearly separable ‚Äî meaning there exists a straight line (or hyperplane) that divides the two classes without any misclassification.\n",
    "\n",
    "It strictly enforces that every data point must be correctly classified and lie outside or on the margin boundaries.\n",
    "\n",
    "\n",
    "A Soft Margin SVM allows some flexibility ‚Äî it accepts that a few points might be:\n",
    "\n",
    "On the wrong side of the margin, or even\n",
    "\n",
    "Misclassified entirely.\n",
    "\n",
    "It introduces slack variables (Œæ·µ¢ ‚â• 0) to measure how much each point violates the margin constraint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157d614-6472-406b-889a-7acaa6bc676f",
   "metadata": {},
   "source": [
    "3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
    "explain its use case.\n",
    "The Kernel Trick allows SVMs to separate data that is not linearly separable by implicitly mapping the data into a higher-dimensional space, without ever computing that mapping directly.\n",
    "\n",
    "Imagine you have data that looks like this in 2D:\n",
    "\n",
    "üü† Inner circle (Class 1)\n",
    "üîµ Outer ring (Class 2)\n",
    "\n",
    "You cannot draw a straight line to separate them in 2D.\n",
    "\n",
    "But if you lift the data into a higher dimension (e.g., add a new feature like \n",
    "ùëß\n",
    "=\n",
    "ùë•\n",
    "2\n",
    "+\n",
    "ùë¶\n",
    "2\n",
    "z=x\n",
    "2\n",
    "+y\n",
    "2\n",
    "), the circular pattern becomes linearly separable in that higher-dimensional space.\n",
    "\n",
    "üëâ The Kernel Trick does this implicitly, using a mathematical function called a kernel function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7a447-7efc-44b1-ad63-d8f0fdf448de",
   "metadata": {},
   "source": [
    "4. What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
    "The Na√Øve Bayes Classifier is a probabilistic machine learning model based on Bayes‚Äô Theorem.\n",
    "\n",
    "It predicts the probability that a given sample belongs to a particular class based on the values of its features.\n",
    "\n",
    "The algorithm is called ‚Äúna√Øve‚Äù because it makes a strong (and unrealistic) assumption:\n",
    "\n",
    "It assumes that all features are independent of each other, given the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6ef3b-b676-4c93-9351-086a6d1da84f",
   "metadata": {},
   "source": [
    "5.  Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants.\n",
    "When would you use each one?\n",
    "\n",
    "Assumes that the continuous features (numeric values) for each class follow a normal (Gaussian) distribution.\\\n",
    "\n",
    "Example Use Cases:\n",
    "\n",
    "Iris flower classification (using petal/sepal length and width)\n",
    "\n",
    "Medical diagnosis (continuous lab test values)\n",
    "\n",
    "Sensor or signal data classification\n",
    "\n",
    "üßÆ 2. Multinomial Na√Øve Bayes\n",
    "üìò Description:\n",
    "\n",
    "Designed for discrete features ‚Äî typically count data (like word frequencies in text).\n",
    "\n",
    "Assumes that the features represent the number of times a particular event occurs (e.g., how often a word appears in a document).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233af8d-5cac-48fb-91e1-f3019d99dee7",
   "metadata": {},
   "source": [
    "6, Dataset Info:\n",
    "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
    "sklearn.datasets or a CSV file you have.\n",
    "Question 6: Write a Python program to:\n",
    "‚óè Load the Iris dataset\n",
    "‚óè Train an SVM Classifier with a linear kernel\n",
    "‚óè Print the model's accuracy and support vectors.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f62d5bcc-3bf1-4126-b6d3-53f379313ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM Classifier Results ===\n",
      "Accuracy: 100.00%\n",
      "\n",
      "Support Vectors (one per class):\n",
      "[[4.8 3.4 1.9 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.9 2.5 4.5 1.7]]\n",
      "\n",
      "Number of Support Vectors for each class:\n",
      "[ 3 11 11]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data        # Feature matrix\n",
    "y = iris.target      # Labels\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a linear kernel\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=== SVM Classifier Results ===\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nSupport Vectors (one per class):\")\n",
    "print(svm_model.support_vectors_)\n",
    "print(\"\\nNumber of Support Vectors for each class:\")\n",
    "print(svm_model.n_support_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0a62b-8e59-4441-98ba-de3f128141ec",
   "metadata": {},
   "source": [
    "7. Write a Python program to:\n",
    "‚óè Load the Breast Cancer dataset\n",
    "‚óè Train a Gaussian Na√Øve Bayes model\n",
    "‚óè Print its classification report including precision, recall, and F1-score.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9f7d0a-80c0-47ab-8df1-c10eda26e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gaussian Na√Øve Bayes Classifier Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       1.00      0.93      0.96        43\n",
      "      benign       0.96      1.00      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data     # Feature matrix\n",
    "y = breast_cancer.target   # Labels (0 = malignant, 1 = benign)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Gaussian Na√Øve Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"=== Gaussian Na√Øve Bayes Classifier Report ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055b032-2611-4c6f-ae94-99f23ad4f105",
   "metadata": {},
   "source": [
    "8. Write a Python program to:\n",
    "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
    "C and gamma.\n",
    "‚óè Print the best hyperparameters and accuracy.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c9941c-c685-4846-9cbd-3ba2babe81da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM Classifier with GridSearchCV ===\n",
      "Best Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Cross-Validation Accuracy: 71.80%\n",
      "Test Set Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data      # Feature matrix\n",
    "y = wine.target    # Labels\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']   # using RBF kernel for non-linear decision boundaries\n",
    "}\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid = GridSearchCV(svm, param_grid, refit=True, verbose=0, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=== SVM Classifier with GridSearchCV ===\")\n",
    "print(f\"Best Parameters: {grid.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid.best_score_ * 100:.2f}%\")\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab65612-db88-46f7-8ae6-3430346c9a70",
   "metadata": {},
   "source": [
    "9. Write a Python program to:\n",
    "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
    "sklearn.datasets.fetch_20newsgroups).\n",
    "‚óè Print the model's ROC-AUC score for its predictions.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132fe89-d45c-4026-8044-d864d2548435",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load a subset of the 20 Newsgroups dataset (binary classification for simplicity)\n",
    "categories = ['sci.space', 'rec.autos']  # Two distinct categories\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = newsgroups.data     # Text documents\n",
    "y = newsgroups.target   # Labels (0 or 1)\n",
    "\n",
    "# Convert text data into TF-IDF feature vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Multinomial Na√Øve Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Na√Øve Bayes Text Classification (ROC-AUC) ===\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ee1a0-8715-4130-85a6-d0d40af97ded",
   "metadata": {},
   "source": [
    "10. Question 10: Imagine you‚Äôre working as a data scientist for a company that handles\n",
    "email communications.\n",
    "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
    "contain:\n",
    "‚óè Text with diverse vocabulary\n",
    "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
    "‚óè Some incomplete or missing data\n",
    "Explain the approach you would take to:\n",
    "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
    "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
    "‚óè Address class imbalance\n",
    "‚óè Evaluate the performance of your solution with suitable metrics\n",
    "And explain the business impact of your solution.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cae39-e97a-4643-a657-b52db0ac074f",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "Emails are text data that may contain missing fields. Steps include:\n",
    "\n",
    "Handling missing data:\n",
    "\n",
    "Drop emails with no content or fill missing text with a placeholder like an empty string.\n",
    "\n",
    "Text cleaning & normalization:\n",
    "\n",
    "Lowercasing, removing punctuation, numbers, and stopwords.\n",
    "\n",
    "Optional: stemming or lemmatization.\n",
    "\n",
    "Text vectorization:\n",
    "\n",
    "Convert emails to numeric vectors for ML models using:\n",
    "\n",
    "TF-IDF (TfidfVectorizer) ‚Äî accounts for word importance.\n",
    "\n",
    "Count Vectorizer (CountVectorizer) ‚Äî counts word occurrences.\n",
    "\n",
    "2Ô∏è‚É£ Model Choice\n",
    "Na√Øve Bayes\n",
    "\n",
    "Works well with text classification, especially spam detection.\n",
    "\n",
    "Handles high-dimensional sparse data efficiently.\n",
    "\n",
    "Performs well even with small training sets.\n",
    "\n",
    "SVM\n",
    "\n",
    "Can produce higher accuracy with well-separated classes.\n",
    "\n",
    "Works with kernels (linear for text data) but is slower on large datasets.\n",
    "\n",
    "‚úÖ Choice: Start with Multinomial Na√Øve Bayes, because spam detection is mostly about word presence and frequency, and NB handles sparse high-dimensional text efficiently.\n",
    "\n",
    "3Ô∏è‚É£ Handling Class Imbalance\n",
    "\n",
    "Spam is usually less frequent than legitimate emails:\n",
    "\n",
    "Techniques:\n",
    "\n",
    "Class weighting: Adjust the classifier to penalize misclassification of minority class more.\n",
    "\n",
    "Resampling: Oversample minority class (SMOTE) or undersample majority class.\n",
    "\n",
    "Threshold tuning: Adjust the probability threshold for classifying spam.\n",
    "\n",
    "4Ô∏è‚É£ Evaluation Metrics\n",
    "\n",
    "For imbalanced datasets, accuracy is misleading. Prefer:\n",
    "\n",
    "Precision: Fraction of predicted spam that is actually spam (avoids false positives).\n",
    "\n",
    "Recall: Fraction of actual spam detected (avoids false negatives).\n",
    "\n",
    "F1-score: Harmonic mean of precision and recall.\n",
    "\n",
    "ROC-AUC: Probability that the model ranks a random spam email higher than a non-spam email.\n",
    "\n",
    "5Ô∏è‚É£ Business Impact\n",
    "\n",
    "Reducing false negatives: Prevent spam from reaching users ‚Üí better user experience, compliance.\n",
    "\n",
    "Reducing false positives: Avoid misclassifying important emails ‚Üí prevents loss of critical communication.\n",
    "\n",
    "Efficiency: Automated filtering reduces manual inspection costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570402ed-588f-4cb2-8256-7e546087b69c",
   "metadata": {},
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (simulate spam vs non-spam)\n",
    "categories = ['rec.autos', 'sci.space']\n",
    "emails = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
    "X, y = [text or \"\" for text in emails.data], emails.target  # Handle missing text\n",
    "\n",
    "# Split and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf, X_test_tfidf = vectorizer.fit_transform(X_train), vectorizer.transform(X_test)\n",
    "\n",
    "# Compute sample weights for imbalance\n",
    "weights = dict(zip(*[np.unique(y_train), compute_class_weight('balanced', np.unique(y_train), y_train)]))\n",
    "sample_weights = np.array([weights[label] for label in y_train])\n",
    "\n",
    "# Train and predict\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train, sample_weight=sample_weights)\n",
    "y_pred, y_proba = model.predict(X_test_tfidf), model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred, target_names=emails.target_names))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a7b5a-26da-4a3e-a7db-ddcf02328fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
