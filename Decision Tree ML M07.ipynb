{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0817e3be-59ab-4f79-a767-08520f18baaa",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a844e-5423-42dc-be9b-16480c270aab",
   "metadata": {},
   "source": [
    "1.  What is a Decision Tree, and how does it work in the context of classification?\n",
    "-  A Decision Tree is a popular supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it is a predictive model that uses a tree-like structure of decisions to determine the class label of a given data point based on its features.\n",
    "\n",
    "\n",
    "A decision tree is similar to a flowchart where:\n",
    "-  Each internal node represents a decision based on a feature (e.g., \"Temperature > 30°C?\")\n",
    "-  Each branch represents the outcome of that decision (Yes/No)\n",
    "-  Each leaf node represents a final class label or output\n",
    "-  The model makes predictions by traversing the tree from the root node to a leaf node, following the decisions at each branch\n",
    "\n",
    "\n",
    "Building a decision tree involves the following steps:\n",
    "\n",
    "1.  Select the Best Feature to Split:\n",
    "-  The algorithm chooses the feature that best separates the data into distinct classes.\n",
    "\n",
    "\n",
    "Selection is based on criteria such as:\n",
    "-  Gini Impurity\n",
    "-  Entropy / Information Gain\n",
    "\n",
    "2.  Split the Dataset: Based on the selected feature, the dataset is split into subsets.\n",
    "\n",
    "3.  Repeat the Process:\n",
    "\n",
    "- The splitting process continues recursively for each subset until:\n",
    "- All samples in a node belong to the same class.\n",
    "- No more features are left to split\n",
    "- A stopping condition (e.g., maximum depth) is reached.\n",
    "\n",
    "4. Label the Leaf Nodes: Each leaf node is assigned the class that occurs most frequently in that subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5594c6f-0ffb-4032-b4c9-ef5a62f487a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7023b0e-3fd5-40d7-9d52-0b58cdec22e8",
   "metadata": {},
   "source": [
    "2.  Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "\n",
    "\n",
    "How do they impact the splits in a Decision Tree?\n",
    "\n",
    "\n",
    "-  When constructing a Decision Tree, the algorithm must decide which feature and threshold to split on at each node. The goal is to choose the split that results in the purest possible child nodes — i.e., nodes where samples mostly belong to a single class.\n",
    "-  To measure how pure or impure a node is, we use impurity measures. Two commonly used impurity measures are Gini Impurity and Entropy (Information Gain).\n",
    "-  Gini impurity measures the probability of incorrectly classifying a randomly chosen element from the dataset if it was randomly labeled according to the class distribution in that node.\n",
    "-  Formula:\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( C \\) = number of classes  \n",
    "- \\( p_i \\) = probability of an instance belonging to class \\( i \\)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- \\( Gini = 0 \\): The node is **pure** (all samples belong to one class).  \n",
    "- Higher \\( Gini \\) → Higher impurity.\n",
    "\n",
    "\n",
    "- Both Gini and Entropy are used to evaluate how good a split is.\n",
    "- The decision tree algorithm tries different features and thresholds and chooses the split that produces child nodes with the lowest impurity\n",
    "- Lower impurity = more homogeneous classes = better classification performance\n",
    "\n",
    "\n",
    "Although both measures generally yield similar trees:\n",
    "- Gini Impurity is computationally faster and slightly more sensitive to class distribution\n",
    "- Entropy is more theoretically grounded (from information theory) but slightly slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a1aa1-8ebb-4b17-8568-1e7ccfcdc317",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fbd45-59a9-412e-9856-b65c86011ff1",
   "metadata": {},
   "source": [
    "3.  What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "\n",
    "\n",
    "Introduction:\n",
    "Decision trees are powerful machine learning models, but they are prone to overfitting if allowed to grow without constraints. Overfitting occurs when the tree becomes too complex and starts to memorize the training data rather than generalizing to unseen data. To prevent this, pruning techniques are used. Pruning reduces the size of the decision tree by removing branches or nodes that provide little or no predictive power. There are two main types of pruning: pre-pruning and post-pruning.\n",
    "\n",
    "Pre-Pruning (Early Stopping):\n",
    "Pre-pruning, also known as early stopping, is a technique where the growth of the decision tree is stopped early before it becomes too deep or complex. Instead of allowing the tree to fully expand, the algorithm uses certain stopping criteria during the training process to decide whether to continue splitting a node. Common conditions include:\n",
    "- Maximum depth: limiting how deep the tree can grow.\n",
    "- Minimum samples per node: stopping splits if a node contains fewer samples than a defined threshold.\n",
    "- Minimum impurity decrease: stopping if further splits do not significantly reduce impurity.\n",
    "\n",
    "Practical Advantage:\n",
    "Pre-pruning helps reduce training time and computational cost because the tree does not grow unnecessarily large. It also reduces the risk of overfitting by controlling the model's complexity from the start.\n",
    "\n",
    "Post-Pruning (Pruning After Tree Construction):\n",
    "Post-pruning is a technique where the decision tree is first allowed to grow fully without any constraints. After the complete tree is built, branches that have little importance or do not improve model accuracy are removed. This is typically done by evaluating the performance of subtrees on a validation dataset and pruning those that lead to overfitting. Post-pruning can be done using techniques like cost complexity pruning or reduced error pruning.\n",
    "\n",
    "Practical Advantage:\n",
    "Post-pruning often results in a more accurate and generalized model. Since the pruning decisions are based on the actual performance of the tree, it allows the algorithm to retain useful splits while removing only those that do not contribute to predictive power.\n",
    "\n",
    "Conclusion:\n",
    "Both pre-pruning and post-pruning aim to prevent overfitting and improve the generalization ability of decision trees. Pre-pruning stops the tree from becoming too complex during training, saving time and computational resources. Post-pruning, on the other hand, refines the fully grown tree and often results in higher accuracy by removing unnecessary branches. The choice between them depends on the problem, dataset size, and computational constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9bd6e-7ad0-48d6-92a6-8ac0069e311b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78514a8f-4f79-4a9b-9dfa-0a9b78fbe01f",
   "metadata": {},
   "source": [
    "4.  What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "\n",
    "\n",
    "Introduction:\n",
    "Information Gain is a key concept used in decision tree algorithms to decide which feature to split on at each node. It is based on the idea of entropy from information theory, which measures the amount of randomness or impurity in a dataset. The goal of a decision tree is to create nodes that are as pure as possible, meaning they contain samples from only one class. Information Gain helps in selecting the feature that results in the highest reduction in impurity after a split.\n",
    "\n",
    "Definition:\n",
    "Information Gain measures the reduction in entropy (or impurity) after splitting the dataset based on a particular feature. In other words, it quantifies how much information about the class label is gained by knowing the value of a specific feature.\n",
    "\n",
    "Formula:\n",
    "The formula for Information Gain is:\n",
    "\n",
    "$$\n",
    "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( S \\): the original dataset\n",
    "- \\( A \\): the attribute (feature) on which we split\n",
    "- \\( S_v \\): the subset of \\( S \\) for which attribute \\( A \\) has value \\( v \\)\n",
    "- \\( |S_v| \\): number of samples in subset \\( S_v \\)\n",
    "- \\( |S| \\): total number of samples\n",
    "- \\( Entropy(S) \\): impurity of the original dataset\n",
    "\n",
    "Interpretation:\n",
    "- A higher information gain means the split has produced purer child nodes.\n",
    "- A lower information gain means the split has not significantly improved purity.\n",
    "\n",
    "Importance in Choosing the Best Split:\n",
    "1. Feature Selection: Information Gain helps the decision tree algorithm choose the most informative feature for splitting at each node.\n",
    "2. Improved Classification: By selecting the split with the highest information gain, the tree quickly reduces uncertainty and improves prediction accuracy.\n",
    "3. Efficient Tree Growth: It ensures the tree grows in a way that maximizes information at each step, leading to smaller, more efficient, and more interpretable trees.\n",
    "4. Reduction of Overfitting: Splits based on information gain lead to better generalization by focusing on meaningful attributes rather than irrelevant ones.\n",
    "\n",
    "Conclusion:\n",
    "Information Gain is a crucial criterion for building decision trees because it measures how well a feature separates the data into different classes. By selecting splits that maximize information gain, the algorithm ensures that each decision made by the tree significantly reduces uncertainty, resulting in a more accurate and efficient classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ca8f6-6b1f-4cb5-bcce-225c1ad22a7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ceb66-7361-4c3c-9669-b7ced9cc850a",
   "metadata": {},
   "source": [
    "5.   What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "\n",
    "\n",
    "Introduction:\n",
    "Decision trees are widely used supervised machine learning algorithms that classify data by splitting it based on feature values. Due to their simplicity, interpretability, and ability to handle different types of data, they are used in many real-world applications across various industries.\n",
    "\n",
    "Common Real-World Applications:\n",
    "\n",
    "1. Medical Diagnosis:\n",
    "   - Decision trees are used in healthcare to diagnose diseases based on patient symptoms, medical history, and test results.\n",
    "   - Example: Predicting whether a patient has diabetes or heart disease.\n",
    "\n",
    "2. Credit Scoring and Risk Assessment:\n",
    "   - Financial institutions use decision trees to evaluate the creditworthiness of applicants by analyzing factors like income, age, debt, and payment history.\n",
    "   - Example: Deciding whether to approve a loan application.\n",
    "\n",
    "3. Fraud Detection:\n",
    "   - Decision trees help identify fraudulent transactions by learning patterns from historical data.\n",
    "   - Example: Detecting credit card fraud based on transaction behavior.\n",
    "\n",
    "4. Customer Segmentation and Marketing:\n",
    "   - Businesses use decision trees to segment customers and target marketing campaigns more effectively.\n",
    "   - Example: Predicting which customers are likely to buy a product.\n",
    "\n",
    "5. Manufacturing and Quality Control:\n",
    "   - In manufacturing, decision trees help in identifying defective products and optimizing production processes.\n",
    "   - Example: Predicting whether a product meets quality standards based on sensor data.\n",
    "\n",
    "6. Recommendation Systems:\n",
    "   - Decision trees can be used in recommendation engines to suggest products or services based on user preferences and behavior.\n",
    "   - Example: Suggesting movies or products to customers on e-commerce platforms.\n",
    "\n",
    "Main Advantages of Decision Trees:\n",
    "1. Easy to Understand and Interpret:\n",
    "   - The tree structure is simple and visually intuitive, making it easy to explain results to non-technical stakeholders.\n",
    "2. Handles Different Data Types:\n",
    "   - Can process both numerical and categorical features without requiring scaling.\n",
    "3. Requires Little Data Preprocessing:\n",
    "   - No need for normalization or standardization of data.\n",
    "4. Works Well with Non-linear Relationships:\n",
    "   - Can model complex decision boundaries without requiring a mathematical equation.\n",
    "\n",
    "Main Limitations of Decision Trees:\n",
    "1. Overfitting:\n",
    "   - Decision trees can become too complex and fit noise in the training data, reducing their performance on new data.\n",
    "2. Instability:\n",
    "   - Small changes in the data can lead to significantly different tree structures.\n",
    "3. Bias Toward Features with Many Levels:\n",
    "   - Features with many categories might dominate splits even if they are not the most informative.\n",
    "4. Less Effective for Continuous Predictions:\n",
    "   - Decision trees are more suitable for classification than regression unless combined with ensemble methods like Random Forests.\n",
    "\n",
    "Conclusion:\n",
    "Decision trees are powerful and versatile tools with wide-ranging real-world applications, from healthcare and finance to marketing and manufacturing. Their ease of use and interpretability make them especially valuable for decision-making tasks. However, they must be used carefully to avoid overfitting and instability, and they are often improved when used as part of ensemble methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfff13-c64c-4607-aec9-b1273529acc8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4767932-bb8e-47e8-af6a-677ee22faf54",
   "metadata": {},
   "source": [
    "Dataset Info:\n",
    "-  Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
    "-  Boston Housing Dataset for regression tasks\n",
    "\n",
    "\n",
    "(sklearn.datasets.load_boston() or provided CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e85117-5333-48b1-a5c4-c12ab04a2f88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c9be8-fb68-4c6f-a6e4-ac32b5b5f854",
   "metadata": {},
   "source": [
    "6. Write a Python program to:\n",
    "-  Load the Iris Dataset\n",
    "-  Train a Decision Tree Classifier using the Gini criterion\n",
    "-  Print the model’s accuracy and feature importances\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce683b8-d06a-4687-93fd-4c784c4498fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data        # Features\n",
    "y = iris.target      # Labels\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Train a Decision Tree Classifier using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Output results\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(iris.feature_names, feature_importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d541650-2421-4fa8-a7f3-fe9205f477a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76afc2e-1ce3-4279-a755-de946891f38d",
   "metadata": {},
   "source": [
    "7.  Write a Python program to:\n",
    "-  Load the Iris Dataset\n",
    "-  Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a34011df-1489-440e-b4c5-56d499e582d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
      "Accuracy of Fully-Grown Decision Tree: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Train a Decision Tree Classifier with max_depth=3\n",
    "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_limited.fit(X_train, y_train)\n",
    "y_pred_limited = clf_limited.predict(X_test)\n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
    "\n",
    "# Step 4: Train a fully-grown Decision Tree Classifier (no max_depth)\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# Step 5: Print and compare accuracies\n",
    "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_limited:.4f}\")\n",
    "print(f\"Accuracy of Fully-Grown Decision Tree: {accuracy_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d9ffe-b5d2-4ea7-bbf5-caa3a1fb46d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d87e77-3376-4d2c-a4cc-384407218c6c",
   "metadata": {},
   "source": [
    "8.  Write a Python program to:\n",
    "-  Load the California Housing dataset from sklearn\n",
    "-  Train a Decision Tree Regressor\n",
    "-  Print the Mean Squared Error (MSE) and feature importances\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08ca587a-2fc0-4976-9c1d-09c78b1ba1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5280096503174904\n",
      "Feature Importances:\n",
      "MedInc: 0.5235\n",
      "HouseAge: 0.0521\n",
      "AveRooms: 0.0494\n",
      "AveBedrms: 0.0250\n",
      "Population: 0.0322\n",
      "AveOccup: 0.1390\n",
      "Latitude: 0.0900\n",
      "Longitude: 0.0888\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Load the California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Train a Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "feature_importances = regressor.feature_importances_\n",
    "\n",
    "# Step 6: Print results\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(california.feature_names, feature_importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740470a9-558d-4068-9e59-773ea5cafac5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d58273-679f-41f6-afa5-5b2fd7b1ec13",
   "metadata": {},
   "source": [
    "9.  Write a Python program to:\n",
    "-  Load the Iris Dataset\n",
    "-  Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "-  Print the best parameters and the resulting model accuracy\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7f77b01-64a7-4d82-885e-7164a0ee6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
      "Model Accuracy with Best Parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Define the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Step 4: Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# Step 5: Use GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Get the best parameters and evaluate the model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Step 7: Print results\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Model Accuracy with Best Parameters:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c220eab-4468-4423-96ab-6bade1d48d75",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa8410-3efd-46c6-807a-9db28055bcca",
   "metadata": {},
   "source": [
    "10.   Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
    "- Handle the missing values\n",
    "- Encode the categorical features\n",
    "- Train a Decision Tree model\n",
    "- Tune its hyperparameters\n",
    "- Evaluate its performance\n",
    "\n",
    "\n",
    "And describe what business value this model could provide in the real-world setting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Step 1: Handle Missing Values\n",
    "- Identify missing data: Check each column for missing values using methods like `.isnull().sum()`.\n",
    "- Decide on strategy:\n",
    "  - Numerical features: Fill missing values using mean, median, or predictive imputation (e.g., KNN Imputer).\n",
    "  - Categorical features: Fill missing values with the mode or create a special category like `\"Unknown\"`.\n",
    "- Rationale: Ensures the model can learn effectively without bias from missing values.\n",
    "\n",
    "\n",
    "Step 2: Encode Categorical Features\n",
    "- Identify categorical variables: For example, gender, blood type, patient region.\n",
    "- Encoding methods:\n",
    "  - Label Encoding: Converts ordinal categories into numerical codes.\n",
    "  - One-Hot Encoding: Converts nominal categories into binary vectors.\n",
    "- Rationale: Decision Trees require numerical inputs to create splits on features.\n",
    "\n",
    "\n",
    "Step 3: Train a Decision Tree Model\n",
    "- Split data: Divide dataset into training and testing sets (e.g., 70%-30%).\n",
    "- Initialize the model: Use `DecisionTreeClassifier()` from scikit-learn.\n",
    "- Train the model: Fit the model using `.fit(X_train, y_train)`.\n",
    "- Rationale: Decision Trees handle mixed data types and non-linear relationships effectively.\n",
    "\n",
    "\n",
    "Step 4: Tune Hyperparameters\n",
    "- Key hyperparameters:\n",
    "  - `max_depth`: Maximum depth to prevent overfitting.\n",
    "  - `min_samples_split` / `min_samples_leaf`: Minimum samples required for splitting or forming a leaf.\n",
    "  - `criterion`: Splitting measure (`gini` or `entropy`).\n",
    "- Tuning method: Use `GridSearchCV` or `RandomizedSearchCV` to test combinations of hyperparameters and select the best.\n",
    "- Rationale: Optimized parameters improve generalization and interpretability.\n",
    "\n",
    "\n",
    "Step 5: Evaluate Model Performance\n",
    "- Metrics to consider:\n",
    "  - Accuracy\n",
    "  - Precision and Recall (important to minimize false positives/negatives)\n",
    "  - F1-Score (balance between precision and recall)\n",
    "  - ROC-AUC (discrimination capability for binary outcomes)\n",
    "- Validation: Evaluate on a separate test set or using cross-validation.\n",
    "- Rationale: Ensures the model is reliable and meets clinical safety standards.\n",
    "\n",
    "\n",
    "Step 6: Business Value\n",
    "- Early disease detection: Identify high-risk patients proactively.\n",
    "- Resource allocation: Prioritize tests and treatments for likely cases.\n",
    "- Decision support: Assist doctors with an interpretable, data-driven model.\n",
    "- Cost reduction: Reduce unnecessary tests and hospitalizations.\n",
    "- Improved patient outcomes: Enable timely interventions and better care\n",
    "\n",
    "\n",
    "Summary\n",
    "1. Handle missing values to clean the dataset.  \n",
    "2. Encode categorical features for model compatibility.  \n",
    "3. Train a Decision Tree on the training dataset.  \n",
    "4. Tune hyperparameters using `GridSearchCV` for optimal performance.  \n",
    "5. Evaluate the model with metrics like accuracy, precision, recall, and ROC-AUC.  \n",
    "6. Deliver business value by supporting better clinical decisions, improving efficiency, and reducing costs.\n",
    "\n",
    "This approach ensures the model is accurate, interpretable, and clinically useful, which is critical in healthcare settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd10be-c795-4176-9aed-47774bcc85b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac41c7-d2d2-4266-8806-31a55c579d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
