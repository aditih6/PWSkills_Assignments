{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca23c56-70e7-4eb6-9cc3-4cf3bafa7789",
   "metadata": {},
   "source": [
    "# Logistic Regression Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ed948-5e77-46e4-a03e-032a13fc2542",
   "metadata": {},
   "source": [
    "1.  What is Logistic Regression, and how does it differ from Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09664fe1-990c-4f33-86d8-f6758f00e1a4",
   "metadata": {},
   "source": [
    "Logistic Regression is a classification algorithm used to predict the probability of a categorical outcome (usually binary: yes/no, spam/not spam, disease/no disease).\n",
    "\n",
    "How it works: Instead of predicting a continuous value, it predicts the probability that a given input belongs to a certain class.\n",
    "\n",
    "Logistic Regression uses the **logistic (sigmoid) function** to map predictions into a range between 0 and 1:\n",
    "\n",
    "Let\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "\\qquad\\text{and}\\qquad\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "P(y=1\\mid x)=\\sigma(z)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x_1+\\dots+\\beta_n x_n)}}.\n",
    "$$\n",
    "\n",
    "Decision rule with a threshold $\\tau\\in(0,1)$:\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } P(y=1\\mid x)\\ge \\tau,\\\\[6pt]\n",
    "0 & \\text{if } P(y=1\\mid x)< \\tau.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Equivalently (in terms of the linear predictor $z$):\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } z \\ge \\log\\!\\left(\\dfrac{\\tau}{1-\\tau}\\right),\\\\[6pt]\n",
    "0 & \\text{if } z < \\log\\!\\left(\\dfrac{\\tau}{1-\\tau}\\right).\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For the common threshold $\\tau = 0.5$ this simplifies to\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\beta_0+\\beta_1 x_1+\\dots+\\beta_n x_n \\ge 0,\\\\[6pt]\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "If probability > threshold (commonly 0.5), we classify as 1, otherwise as 0.\n",
    "\n",
    "\n",
    "| Feature             | **Linear Regression**                   | **Logistic Regression**                            |\n",
    "| ------------------- | --------------------------------------- | -------------------------------------------------- |\n",
    "| **Type of Problem** | Regression (predicts continuous values) | Classification (predicts categories/probabilities) |\n",
    "| **Output Range**    | $-\\infty$ to $+\\infty$                  | $0$ to $1$ (probability)                           |\n",
    "| **Equation**        | Straight line (linear function)         | Sigmoid curve (logistic function)                  |\n",
    "| **Loss Function**   | Mean Squared Error (MSE)                | Log Loss (Cross-Entropy Loss)                      |\n",
    "| **Interpretation**  | \"What value will $y$ take?\"             | \"What is the probability $y$ belongs to class 1?\"  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bef04-55a8-4874-bb40-d3f9470ffe12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d0567-6fee-489d-90b5-9ac49f9d4630",
   "metadata": {},
   "source": [
    "2.   Explain the role of the Sigmoid function in Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07aee55-7f5e-4e55-8fcd-f2d26dde3d1a",
   "metadata": {},
   "source": [
    "The sigmoid (logistic) function is at the heart of logistic regression.\n",
    "\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{where } z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "\n",
    "Without the sigmoid, the linear model can produce outputs in the range $(-\\infty, +\\infty)$. The sigmoid function squashes these outputs to lie strictly between 0 and 1, which makes them interpretable as probabilities.  \n",
    "\n",
    "For classification, a threshold $\\tau$ is applied:  \n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\sigma(z) \\ge \\tau \\\\\n",
    "0 & \\text{if } \\sigma(z) < \\tau\n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "Typically, $\\tau = 0.5$. This means if the probability is at least 0.5, the model predicts class 1, otherwise class 0.  \n",
    "\n",
    "The sigmoid is also differentiable, and its derivative has a convenient form  \n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)\\,(1 - \\sigma(z)),\n",
    "$$  \n",
    "\n",
    "which makes optimization using gradient descent efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8931e04-aa73-44b3-8c31-a495f295ee39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930960fb-014a-4023-906f-d75020eeed67",
   "metadata": {},
   "source": [
    "3.   What is Regularization in Logistic Regression and why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139be6fb-89e4-409f-9300-a8bc78d6846d",
   "metadata": {},
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting and improve the generalization ability of the model. In logistic regression, the model tries to find the best coefficients (weights) for the features to minimize the log loss function. However, when the model is very complex or the number of features is large, the coefficients can take very high values, causing the model to fit the training data too closely. This leads to poor performance on unseen data.\n",
    "\n",
    "The standard cost function for logistic regression is\n",
    "\n",
    "$$\n",
    "J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\Big[y^{(i)} \\log(h_\\beta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\beta(x^{(i)}))\\Big],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "h_\\beta(x) = \\frac{1}{1 + e^{-\\beta^T x}}\n",
    "$$\n",
    "\n",
    "is the sigmoid function.\n",
    "\n",
    "To control the size of the coefficients, a penalty term is added to this cost function. This is called regularization. The two most common types are:\n",
    "\n",
    "1. **L2 Regularization (Ridge)**  \n",
    "   The penalty is the sum of squares of the coefficients:  \n",
    "\n",
    "   $$\n",
    "   J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\\beta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\beta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\beta_j^2\n",
    "   $$\n",
    "\n",
    "   This shrinks the coefficients but does not make them exactly zero.\n",
    "\n",
    "2. **L1 Regularization (Lasso)**  \n",
    "   The penalty is the sum of the absolute values of the coefficients:  \n",
    "\n",
    "   $$\n",
    "   J(\\beta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\\beta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\beta(x^{(i)}))] + \\frac{\\lambda}{m}\\sum_{j=1}^n |\\beta_j|\n",
    "   $$\n",
    "\n",
    "   This can shrink some coefficients exactly to zero, performing feature selection.\n",
    "\n",
    "The parameter $\\lambda$ (regularization strength) controls the amount of penalty.  \n",
    "- If $\\lambda$ is very large, the model will underfit because coefficients are heavily penalized.  \n",
    "- If $\\lambda$ is very small (close to 0), the effect of regularization disappears and the model may overfit.  \n",
    "\n",
    "Regularization is needed because:  \n",
    "- It prevents overfitting by avoiding very large coefficients.  \n",
    "- It reduces variance and improves stability of the model.  \n",
    "- It improves prediction accuracy on unseen test data.  \n",
    "- In the case of L1 regularization, it also performs feature selection by eliminating less important features.  \n",
    "\n",
    "In summary, regularization in logistic regression ensures that the model remains simple, interpretable, and generalizes well to new data, rather than memorizing the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e7cca-1f9b-4878-8e9c-a9e4bea64b66",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8bc63-07bd-494d-bce2-69f2528bca5b",
   "metadata": {},
   "source": [
    "4.   What are some common evaluation metrics for classification models, and why are they important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e18bf7-f6ea-435b-a313-fc8231bfe525",
   "metadata": {},
   "source": [
    "Evaluation metrics for classification models are used to measure how well the model performs in predicting categorical outcomes. They are important because accuracy alone is not always sufficient, especially when data is imbalanced. Common evaluation metrics include:\n",
    "\n",
    "1. **Accuracy**  \n",
    "   The proportion of correctly classified examples.  \n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   $$\n",
    "   Useful when classes are balanced, but misleading when data is skewed.\n",
    "\n",
    "2. **Precision**  \n",
    "   The proportion of positive predictions that are actually correct.  \n",
    "   $$\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   $$\n",
    "   Important when the cost of false positives is high (e.g., spam detection).\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate)**  \n",
    "   The proportion of actual positives that are correctly identified.  \n",
    "   $$\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "   Important when the cost of false negatives is high (e.g., disease detection).\n",
    "\n",
    "4. **F1 Score**  \n",
    "   The harmonic mean of precision and recall.  \n",
    "   $$\n",
    "   F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "   Balances precision and recall, useful when both are important.\n",
    "\n",
    "5. **ROC Curve and AUC (Area Under Curve)**  \n",
    "   ROC plots the True Positive Rate against the False Positive Rate.  \n",
    "   AUC measures the area under this curve (values closer to 1 indicate better models).  \n",
    "   Useful for comparing classifiers across different thresholds.\n",
    "\n",
    "These metrics are important because they provide a more complete picture of model performance. Depending on the application, one metric may be more relevant than others. For example, in medical diagnosis, recall is critical, while in spam detection, precision may be more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a38a79-bd80-4b14-8352-9dce00b10786",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18c684-e077-4b46-b04a-6d861cf24747",
   "metadata": {},
   "source": [
    "5.   Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
    "\n",
    "\n",
    "(Use Dataset from sklearn package)\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614c30a2-7238-4a4b-8090-4b1be6f53e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns] \n",
      "\n",
      "Accuracy of Logistic Regression model on test data: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset from sklearn\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X = df.drop('target', axis = 1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter = 5000)  # increased max_iter for convergence\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Logistic Regression model on test data:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4faa06-b0b2-45c9-8012-644aafe19eed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b2028-1619-4e22-9b49-574a9a101db6",
   "metadata": {},
   "source": [
    "6.   Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
    "\n",
    "\n",
    "(Use Dataset from sklearn package)\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b254ef-c87f-4ef0-9104-0324cf599415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns] \n",
      "\n",
      "Model Coefficients:\n",
      "[[ 0.97609898  0.22601681 -0.36871568  0.02641781 -0.15437034 -0.22984154\n",
      "  -0.52153366 -0.27654667 -0.22362021 -0.03659382 -0.09467025  1.385945\n",
      "  -0.1632311  -0.08914811 -0.02226473  0.04672319 -0.04406142 -0.03169923\n",
      "  -0.03345026  0.01150393  0.09336423 -0.51496079 -0.01689119 -0.0165488\n",
      "  -0.30529165 -0.76071927 -1.41534453 -0.50006811 -0.73410842 -0.1004864 ]]\n",
      "\n",
      "Intercept:\n",
      "[29.62817703]\n",
      "\n",
      "Accuracy of Logistic Regression with L2 regularization: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (Breast Cancer dataset from sklearn)\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Logistic Regression model with L2 regularization (default)\n",
    "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Coefficients:\")\n",
    "print(model.coef_)\n",
    "print(\"\\nIntercept:\")\n",
    "print(model.intercept_)\n",
    "print(\"\\nAccuracy of Logistic Regression with L2 regularization:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d08d11-a2d6-4934-bb6c-b21058874daf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96dd6d9-4999-4d9f-89ad-1d4c674ee6de",
   "metadata": {},
   "source": [
    "7.   Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
    "\n",
    "\n",
    "(Use Dataset from sklearn package)\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7186ce80-8c01-4463-9275-5b8d33bb1a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0   \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.89      0.94         9\n",
      "   virginica       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset (Iris dataset for multiclass classification)\n",
    "data = load_iris()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Logistic Regression model with One-vs-Rest strategy\n",
    "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f1447-eabf-48c4-aff2-69afd52e4ba0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58335-dd81-4504-b5d7-ed0b9e92e829",
   "metadata": {},
   "source": [
    "8.   Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
    "\n",
    "\n",
    "(Use Dataset from sklearn package)\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abed21ec-dcd6-43a8-8447-1f8b68e7de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns] \n",
      "\n",
      "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
      "Best Cross-Validation Accuracy: 0.9670329670329672\n",
      "Test Set Accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=5000, solver='liblinear')  \n",
    "# Note: 'liblinear' supports both l1 and l2 penalties\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],   # inverse of regularization strength\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f08e2-d504-4403-b8ef-2d87d84cea4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6d90d-b011-4f98-a53d-58edcd83e294",
   "metadata": {},
   "source": [
    "9.   Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
    "\n",
    "\n",
    "(Use Dataset from sklearn package)\n",
    "\n",
    "\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e00cfecf-3717-4ef3-8e1e-0eb21f976a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns] \n",
      "\n",
      "Accuracy without feature scaling: 0.956140350877193\n",
      "Accuracy with feature scaling: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Model without scaling\n",
    "# -----------------------------\n",
    "model_no_scaling = LogisticRegression(max_iter=5000)\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# -----------------------------\n",
    "# Standardize the features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model with scaling\n",
    "model_scaled = LogisticRegression(max_iter=5000)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# -----------------------------\n",
    "# Print results\n",
    "# -----------------------------\n",
    "print(\"Accuracy without feature scaling:\", acc_no_scaling)\n",
    "print(\"Accuracy with feature scaling:\", acc_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab95dd-6ba5-46aa-8773-88352b685d0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e1a1e-dbb0-4e2c-b9af-ed32f5dc1fb0",
   "metadata": {},
   "source": [
    "10.  Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af722ef7-c5a9-40e4-a840-b0bd8bd43a67",
   "metadata": {},
   "source": [
    "To predict which customers will respond to a marketing campaign using Logistic Regression on an imbalanced dataset (only 5% responders), the approach would involve the following steps:\n",
    "\n",
    "1. **Data Exploration and Cleaning**  \n",
    "   - Examine the dataset for missing values, outliers, and inconsistent entries.  \n",
    "   - Encode categorical variables using one-hot encoding or ordinal encoding as appropriate.  \n",
    "   - Remove irrelevant features or reduce dimensionality if necessary.\n",
    "\n",
    "2. **Feature Scaling**  \n",
    "   - Standardize numerical features using `StandardScaler` because Logistic Regression is sensitive to feature scales.  \n",
    "   - Scaling ensures that the optimization converges efficiently and coefficients are comparable.\n",
    "\n",
    "3. **Handling Imbalanced Classes**  \n",
    "   - Since only 5% of customers respond, the dataset is highly imbalanced.  \n",
    "   - Techniques to address imbalance:  \n",
    "     - **Resampling**: Oversample the minority class (e.g., using SMOTE) or undersample the majority class.  \n",
    "     - **Class weights**: Use `class_weight='balanced'` in `LogisticRegression` to penalize misclassification of the minority class more heavily.  \n",
    "   - This ensures the model does not simply predict all customers as non-responders.\n",
    "\n",
    "4. **Train-Test Split**  \n",
    "   - Split the data into training and testing sets (e.g., 80-20 split) using stratified sampling to preserve the class distribution.  \n",
    "\n",
    "5. **Hyperparameter Tuning**  \n",
    "   - Use `GridSearchCV` or `RandomizedSearchCV` to tune hyperparameters such as:  \n",
    "     - Regularization strength `C`  \n",
    "     - Penalty type (`l1` or `l2`)  \n",
    "     - Solver choice (`liblinear` for small datasets, `saga` for large datasets)  \n",
    "   - Include cross-validation to select hyperparameters that generalize well.\n",
    "\n",
    "6. **Model Training**  \n",
    "   - Train the Logistic Regression model with the chosen hyperparameters and class balancing strategy.  \n",
    "   - Monitor convergence and ensure the model has not overfitted to the training data.\n",
    "\n",
    "7. **Evaluation Metrics**  \n",
    "   - Accuracy is not sufficient for imbalanced data. Use metrics that focus on the minority class:  \n",
    "     - **Precision**: Fraction of predicted responders who are actually responders.  \n",
    "     - **Recall (Sensitivity)**: Fraction of actual responders correctly identified.  \n",
    "     - **F1-score**: Harmonic mean of precision and recall.  \n",
    "     - **ROC-AUC**: Measures model's ability to distinguish responders from non-responders across thresholds.  \n",
    "   - Plot a **Precision-Recall curve** to analyze trade-offs between precision and recall.\n",
    "\n",
    "8. **Threshold Tuning**  \n",
    "   - Adjust the decision threshold based on business needs:  \n",
    "     - If false negatives (missing potential responders) are costly, lower the threshold to increase recall.  \n",
    "     - If false positives (sending unnecessary promotions) are costly, raise the threshold to increase precision.\n",
    "\n",
    "9. **Business Considerations**  \n",
    "   - Ensure the model outputs interpretable probabilities so marketing teams can prioritize high-probability responders.  \n",
    "   - Monitor model performance periodically and retrain with new customer behavior data to maintain accuracy.  \n",
    "\n",
    "10. **Deployment**  \n",
    "    - Integrate the model into the marketing pipeline to score customers and guide campaign targeting.  \n",
    "    - Track campaign response rates and adjust the model or threshold based on real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ef530-f474-4583-9c75-d4719cc27d3d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
